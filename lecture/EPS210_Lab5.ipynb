{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hunkim98/earth_science/blob/main/lecture/EPS210_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Harvard EPS-210 AI for Earth and Planetary Science\n",
        "\n",
        "Instructor: Mostafa Mouasvi\n",
        "\n",
        "# **Lab 5**:\n",
        "\n",
        "**Activity 1**: Building Change Detection from Satellite Imagery\n",
        "\n",
        "**Activity 2**: Martian Crater Detection with YOLO\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "j_oeFxpYhPlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #A51C30 0%, #1E5A96 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;\">\n",
        "<h1 style=\"color: white; margin: 0;\"> Building Change Detection from Satellite Imagery</h1>\n",
        "<h2 style=\"color: #f0d0d0; margin-top: 10px;\">Using Siamese Multi-Scale CNNs</h2>\n",
        "\n",
        "## üìã Overview\n",
        "\n",
        "| | |\n",
        "|---|---|\n",
        "| **Topic** | Building Change Detection Using Siamese CNNs |\n",
        "| **Dataset** | LEVIR-CD (Building Change Detection Benchmark) |\n",
        "| **Framework** | PyTorch |\n",
        "| **Duration** | ~ 45 Minutes |\n",
        "| **Prerequisites** | Basic Python, intro to CNNs, familiarity with PyTorch |\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "1. Understand the **Siamese network** architecture for bi-temporal image comparison\n",
        "2. Implement a **Deep Siamese Multi-Scale CNN (DSMS-FCN)** for pixel-wise change detection\n",
        "3. Train and evaluate a model on the **LEVIR-CD** building change dataset\n",
        "4. Interpret change detection results and compute standard evaluation metrics (F1, IoU, OA)\n",
        "5. Connect remote sensing change detection to real-world applications: urban growth monitoring and post-disaster damage assessment\n",
        "\n",
        "### Key References\n",
        "\n",
        "- [DSMSCN](https://github.com/ChenHongruixuan/DSMSCN) ‚Äî Deep Siamese Multi-Scale Convolutional Network (Chen et al., 2019)\n",
        "- [SNUNet-CD / Siam-NestedUNet](https://github.com/likyoo/Siam-NestedUNet) ‚Äî Densely Connected Siamese Network (Fang et al., 2021)\n",
        "- [KPCAMNet](https://github.com/ChenHongruixuan/KPCAMNet) ‚Äî Unsupervised Change Detection with Kernel PCA (Chen et al., 2022)\n",
        "- [ChangeDetectionRepository](https://github.com/ChenHongruixuan/ChangeDetectionRepository) ‚Äî Collection of traditional & DL-based CD methods\n",
        "- [Change-Detection-Review](https://github.com/MinZHANG-WHU/Change-Detection-Review) ‚Äî Comprehensive review of AI-based CD methods\n",
        "- [Awesome RS Change Detection](https://github.com/wenhwu/awesome-remote-sensing-change-detection) ‚Äî Datasets, methods, and competitions"
      ],
      "metadata": {
        "id": "jDUcHSYnhdkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. Background\n",
        "\n",
        "## 1.1 Why Change Detection Matters\n",
        "\n",
        "Change detection from satellite imagery is one of the most impactful applications of remote sensing. By comparing images of the same location acquired at different times, we can automatically identify where buildings have appeared, been demolished, or sustained damage. This capability is critical for:\n",
        "\n",
        "- **Urban planning**: tracking city expansion and infrastructure development\n",
        "- **Disaster response**: assessing earthquake, hurricane, or wildfire damage\n",
        "- **Environmental monitoring**: detecting deforestation, coastal erosion, land-use change\n",
        "\n",
        "Traditional approaches relied on hand-crafted features and pixel-level differencing, but these struggle with the complex heterogeneity of VHR satellite images where illumination changes, seasonal variation, and registration errors produce false alarms.\n",
        "\n",
        "## 1.2 Siamese Networks for Change Detection\n",
        "\n",
        "A **Siamese Neural Network** (sometimes called a twin neural network) is a unique architecture designed not to classify an input, but to differentiate or find similarities between two different inputs.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/smousavi05/Harvard-EPS-210/raw/main/figures/siames_nn.png\" width=\"700\">\n",
        "</p>\n",
        "\n",
        " For change detection:\n",
        "- **Input**: Two images of the same area at times T‚ÇÅ and T‚ÇÇ\n",
        "- **Feature Extraction**: Each network transforms its input into a low-dimensional vector, called an embedding.\n",
        "- **Shared encoder**: Maps both images into the same feature space\n",
        "- **Difference module**: Computes |F(T‚ÇÅ) ‚àí F(T‚ÇÇ)| at multiple scales\n",
        "- **Decoder**: Upsamples difference features to produce a pixel-wise change map\n",
        "- **Similarity Measurement**: The outputs are then fed into a loss function (like Triplet Loss or Contrastive Loss) that calculates the distance between the two vectors.\n",
        "\n",
        "    * Small distance: The inputs are very similar.\n",
        "    * Large distance: The inputs are different.\n",
        "\n",
        "The key innovation in this lab is the **Multi-Scale Feature Convolution Unit (MFCU)**, which extracts features at multiple spatial scales (1√ó1, 3√ó3, 5√ó5 kernels) within a single layer, inspired by the DSMSCN architecture (Chen et al., 2019).\n",
        "\n",
        "## 1.3 The LEVIR-CD Dataset\n",
        "\n",
        "We use the **LEVIR-CD** dataset (Chen & Shi, 2020):\n",
        "\n",
        "- **637** pairs of VHR Google Earth images (0.5 m/pixel, 1024√ó1024)\n",
        "- **20 regions** in Texas, USA, spanning 2002‚Äì2018\n",
        "- **31,333** annotated building change instances\n",
        "- Binary labels: 1 = change (new construction/demolition), 0 = no change"
      ],
      "metadata": {
        "id": "Jpca-AqEJ0id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. Part 1 ‚Äî Environment Setup & Data Preparation\n",
        "\n",
        "## 2.1 Install Dependencies"
      ],
      "metadata": {
        "id": "ntuz9PvRJ5Eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0yuzPbThEOg"
      },
      "outputs": [],
      "source": [
        "#@title üì¶ Install Required Packages { display-mode: \"form\" }\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q torchmetrics matplotlib scikit-learn scikit-image tqdm gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {DEVICE}')\n",
        "if DEVICE == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ],
      "metadata": {
        "id": "ALaSurM-KBxZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Download the LEVIR-CD Dataset\n",
        "\n",
        "The LEVIR-CD dataset is organized as:\n",
        "```\n",
        "LEVIR-CD/\n",
        "‚îú‚îÄ‚îÄ train/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ A/       # Time 1 images\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ B/       # Time 2 images\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ label/   # Binary change masks\n",
        "‚îú‚îÄ‚îÄ val/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ A/, B/, label/\n",
        "‚îî‚îÄ‚îÄ test/\n",
        "    ‚îú‚îÄ‚îÄ A/, B/, label/\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e-o22HLqhqiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Download, Unzip & Organize LEVIR-CD { display-mode: \"form\" }\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import gdown\n",
        "\n",
        "DATA_ROOT = '/content/LEVIR-CD'\n",
        "\n",
        "if not os.path.exists(DATA_ROOT):\n",
        "    os.makedirs(DATA_ROOT)\n",
        "\n",
        "print(\"üöÄ Starting download from Google Drive...\")\n",
        "\n",
        "file_ids = {\n",
        "    'test.zip': '1UPaZuyYe-JufA6042go7pIvxuiuICN1s',\n",
        "    'train.zip': '1qeyzaXk5ZF7MqVOe1OVxtEd0MnCMzBWf',\n",
        "    'val.zip': '1L78dDgeKSd7UTP2hjWeAnnwTIpAHvMiL'\n",
        "}\n",
        "\n",
        "for filename, file_id in file_ids.items():\n",
        "    output_path = os.path.join(DATA_ROOT, filename)\n",
        "    if not os.path.exists(output_path):\n",
        "        print(f\"   Downloading {filename}...\")\n",
        "        gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "    else:\n",
        "        print(f\"   {filename} already exists, skipping download.\")\n",
        "\n",
        "# 4. Clean, Unzip, and Reorganize\n",
        "def clean_and_reorganize():\n",
        "    print(\"\\nüßπ Cleaning up old extracted folders to prevent conflicts...\")\n",
        "    for folder in ['A', 'B', 'label', 'train', 'val', 'test']:\n",
        "        path = os.path.join(DATA_ROOT, folder)\n",
        "        if os.path.exists(path) and os.path.isdir(path):\n",
        "            shutil.rmtree(path)\n",
        "\n",
        "    print(\"üì¶ Extracting and organizing files...\")\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        zip_path = os.path.join(DATA_ROOT, f'{split}.zip')\n",
        "        target_dir = os.path.join(DATA_ROOT, split)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(zip_path):\n",
        "            print(f\"   Processing {split}.zip...\")\n",
        "            try:\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(target_dir)\n",
        "\n",
        "                # Handle nested folders\n",
        "                sub_items = os.listdir(target_dir)\n",
        "                nested_folder = os.path.join(target_dir, split)\n",
        "\n",
        "                if split in sub_items and os.path.isdir(nested_folder):\n",
        "                    print(f\"   -> Fixing nested folder structure for {split}...\")\n",
        "                    for item in os.listdir(nested_folder):\n",
        "                        shutil.move(os.path.join(nested_folder, item), target_dir)\n",
        "                    os.rmdir(nested_folder)\n",
        "            except zipfile.BadZipFile:\n",
        "                print(f\"   ‚ö†Ô∏è Warning: {split}.zip appears to be corrupted. skipping.\")\n",
        "        else:\n",
        "             print(f\"   ‚ö†Ô∏è Warning: {split}.zip not found in download.\")\n",
        "\n",
        "    print(\"‚úÖ Reorganization complete.\")\n",
        "\n",
        "clean_and_reorganize()\n",
        "\n",
        "train_path = os.path.join(DATA_ROOT, 'train', 'A')\n",
        "if os.path.exists(train_path):\n",
        "    count = len(os.listdir(train_path))\n",
        "    print(f\"\\nüéâ SUCCESS! Real LEVIR-CD dataset is ready.\")\n",
        "    print(f\"   Training samples found: {count}\")\n",
        "    print(f\"   Location: {train_path}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Error: Data still not found. Please check the /content/LEVIR-CD folder manually.\")"
      ],
      "metadata": {
        "id": "W-Zu3lw_XAWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚úÇÔ∏è Resize Dataset to 128x128 (to prevent RAM Crash) { display-mode: \"form\" }\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Paths\n",
        "ORIGINAL_ROOT = '/content/LEVIR-CD'\n",
        "NEW_ROOT = '/content/LEVIR-CD-128'\n",
        "TARGET_SIZE = (128, 128)\n",
        "\n",
        "def resize_dataset_to_disk(src_root, dst_root, size):\n",
        "    print(f\"üìâ Resizing dataset from {src_root} to {dst_root}...\")\n",
        "\n",
        "    if not os.path.exists(src_root):\n",
        "        raise FileNotFoundError(f\"Original dataset not found at {src_root}!\")\n",
        "\n",
        "    # Process all splits and subfolders\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for subdir in ['A', 'B', 'label']:\n",
        "            src_dir = os.path.join(src_root, split, subdir)\n",
        "            dst_dir = os.path.join(dst_root, split, subdir)\n",
        "\n",
        "            os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "            files = sorted(os.listdir(src_dir))\n",
        "            for fname in tqdm(files, desc=f\"{split}/{subdir}\", leave=False):\n",
        "                if not fname.endswith(('.png', '.jpg', '.tif')):\n",
        "                    continue\n",
        "\n",
        "                # Load\n",
        "                src_path = os.path.join(src_dir, fname)\n",
        "                img = Image.open(src_path)\n",
        "\n",
        "                # Resize (Nearest Neighbor for masks to keep them binary!)\n",
        "                if subdir == 'label':\n",
        "                    img_resized = img.resize(size, resample=Image.NEAREST)\n",
        "                else:\n",
        "                    img_resized = img.resize(size, resample=Image.BILINEAR)\n",
        "\n",
        "                # Save\n",
        "                dst_path = os.path.join(dst_dir, fname)\n",
        "                img_resized.save(dst_path)\n",
        "\n",
        "    print(f\"‚úÖ Resize Complete. New dataset located at: {dst_root}\")\n",
        "\n",
        "# Run the resize\n",
        "resize_dataset_to_disk(ORIGINAL_ROOT, NEW_ROOT, TARGET_SIZE)"
      ],
      "metadata": {
        "id": "jW1xmC1sKaue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Data Loading and Exploration"
      ],
      "metadata": {
        "id": "Pc2--oAtKiUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "DATA_ROOT_FOR_LOADER = NEW_ROOT # Use the 128x128 resized dataset\n",
        "\n",
        "class LEVIRCDDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for LEVIR-CD change detection.\n",
        "\n",
        "    Loads bi-temporal image pairs (A=T1, B=T2) and binary change masks.\n",
        "    Images are normalized using ImageNet statistics.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, split='train', augment=False):\n",
        "        self.augment = augment\n",
        "        self.normalize = T.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "\n",
        "        # Check for standard structure: root/train/A\n",
        "        if os.path.exists(os.path.join(root_dir, split, 'A')):\n",
        "            self.img_A_dir = os.path.join(root_dir, split, 'A')\n",
        "            self.img_B_dir = os.path.join(root_dir, split, 'B')\n",
        "            self.label_dir = os.path.join(root_dir, split, 'label')\n",
        "            # Load all files in the split directory\n",
        "            self.filenames = sorted([f for f in os.listdir(self.img_A_dir)\n",
        "                                     if f.endswith(('.png', '.jpg', '.tif'))])\n",
        "        else:\n",
        "             raise FileNotFoundError(f\"Could not find dataset images in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.filenames[idx]\n",
        "        img_A = Image.open(os.path.join(self.img_A_dir, fname)).convert('RGB')\n",
        "        img_B = Image.open(os.path.join(self.img_B_dir, fname)).convert('RGB')\n",
        "        label = Image.open(os.path.join(self.label_dir, fname)).convert('L')\n",
        "\n",
        "        # Data augmentation (training only)\n",
        "        if self.augment:\n",
        "            # Random horizontal flip\n",
        "            if np.random.random() > 0.5:\n",
        "                img_A = img_A.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                img_B = img_B.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                label = label.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            # Random vertical flip\n",
        "            if np.random.random() > 0.5:\n",
        "                img_A = img_A.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "                img_B = img_B.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "                label = label.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "            # Random 90-degree rotation\n",
        "            if np.random.random() > 0.5:\n",
        "                k = np.random.choice([1, 2, 3])\n",
        "                img_A = img_A.rotate(90 * k)\n",
        "                img_B = img_B.rotate(90 * k)\n",
        "                label = label.rotate(90 * k)\n",
        "\n",
        "        # Convert to tensors\n",
        "        img_A = self.normalize(T.ToTensor()(img_A))\n",
        "        img_B = self.normalize(T.ToTensor()(img_B))\n",
        "        label = (T.ToTensor()(label) > 0.5).float().squeeze(0)\n",
        "\n",
        "        return img_A, img_B, label\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_ds = LEVIRCDDataset(DATA_ROOT_FOR_LOADER, split='train', augment=True)\n",
        "val_ds   = LEVIRCDDataset(DATA_ROOT_FOR_LOADER, split='val',   augment=False)\n",
        "test_ds  = LEVIRCDDataset(DATA_ROOT_FOR_LOADER, split='test',  augment=False)\n",
        "\n",
        "print(f'Training samples:   {len(train_ds)}')\n",
        "print(f'Validation samples: {len(val_ds)}')\n",
        "print(f'Test samples:       {len(test_ds)}')\n"
      ],
      "metadata": {
        "id": "52LGTxX6KboY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Visualize Sample Pairs\n",
        "\n",
        "Let's look at a few bi-temporal image pairs and their change masks:"
      ],
      "metadata": {
        "id": "19DV_u35K5xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_samples(dataset, indices, title=''):\n",
        "    \"\"\"Display bi-temporal image pairs and change masks.\"\"\"\n",
        "    n = len(indices)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std  = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    fig, axes = plt.subplots(n, 3, figsize=(14, 4.5 * n))\n",
        "    if n == 1: axes = axes[np.newaxis, :]\n",
        "\n",
        "    col_titles = ['Time 1 (Before)', 'Time 2 (After)', 'Change Mask (GT)']\n",
        "    for j, t in enumerate(col_titles):\n",
        "        axes[0, j].set_title(t, fontsize=14, fontweight='bold')\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        imgA, imgB, mask = dataset[idx]\n",
        "        imgA_np = np.clip(imgA.permute(1,2,0).numpy() * std + mean, 0, 1)\n",
        "        imgB_np = np.clip(imgB.permute(1,2,0).numpy() * std + mean, 0, 1)\n",
        "\n",
        "        axes[i, 0].imshow(imgA_np)\n",
        "        axes[i, 1].imshow(imgB_np)\n",
        "        axes[i, 2].imshow(mask.numpy(), cmap='hot', vmin=0, vmax=1)\n",
        "\n",
        "        for ax in axes[i]:\n",
        "            ax.axis('off')\n",
        "\n",
        "        # Show change percentage\n",
        "        pct = mask.sum().item() / mask.numel() * 100\n",
        "        axes[i, 2].text(5, 15, f'{pct:.1f}% changed', color='cyan',\n",
        "                        fontsize=11, fontweight='bold',\n",
        "                        bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
        "\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Display 4 random samples\n",
        "np.random.seed(42)\n",
        "sample_indices = np.random.choice(len(train_ds), 4, replace=False)\n",
        "show_samples(train_ds, sample_indices, title='LEVIR-CD Training Samples')"
      ],
      "metadata": {
        "id": "zLUdo6YNKkSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Statistics"
      ],
      "metadata": {
        "id": "27Zz3Qi2LBMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class balance statistics\n",
        "total_pixels = 0\n",
        "change_pixels = 0\n",
        "\n",
        "print('Analyzing class balance (sampling 100 images)...')\n",
        "sample_size = min(100, len(train_ds))\n",
        "for i in tqdm(range(sample_size)):\n",
        "    _, _, mask = train_ds[i]\n",
        "    total_pixels += mask.numel()\n",
        "    change_pixels += mask.sum().item()\n",
        "\n",
        "change_ratio = change_pixels / total_pixels\n",
        "print(f'\\nClass Balance Analysis:')\n",
        "print(f'  Changed pixels:   {change_ratio:.2%}')\n",
        "print(f'  Unchanged pixels: {1 - change_ratio:.2%}')\n",
        "print(f'  Imbalance ratio:  1:{(1 - change_ratio) / change_ratio:.0f}')\n",
        "print(f'\\n‚ö†Ô∏è  Significant class imbalance! This motivates our use of Dice Loss.')"
      ],
      "metadata": {
        "id": "FmgO7AJOK7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: #e8f5e9; border-left: 4px solid #4caf50; padding: 12px; margin: 10px 0; border-radius: 4px;\">\n",
        "<b>‚úÖ Checkpoint:</b> Verify that you can load and display image pairs. You should see RGB satellite images and binary change masks. Notice how changed areas (new buildings) appear as white regions in the mask, and that the changed class is a small fraction of total pixels.\n",
        "</div>"
      ],
      "metadata": {
        "id": "K05FOmj0LKd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3. Part 2 ‚Äî Building the Siamese CNN Model\n",
        "\n",
        "## 3.1 Multi-Scale Feature Convolution Unit (MFCU)\n",
        "\n",
        "The **MFCU** is the building block of our network, inspired by the [DSMSCN architecture](https://github.com/ChenHongruixuan/DSMSCN). It extracts spatial features at multiple scales using **parallel convolution branches** with kernel sizes 1√ó1, 3√ó3, and 5√ó5. The outputs are concatenated and fused with a 1√ó1 convolution.\n",
        "\n",
        "This design (similar to Inception modules) captures both fine-grained building edges and broader contextual information simultaneously."
      ],
      "metadata": {
        "id": "aPxR1YHqLMhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MFCU(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Scale Feature Convolution Unit.\n",
        "\n",
        "    Parallel branches with 1x1, 3x3, and 5x5 kernels capture\n",
        "    features at different spatial scales, then fuse them.\n",
        "\n",
        "    Inspired by: Chen et al. (2019) \"Deep Siamese Multi-scale\n",
        "    Convolutional Network for Change Detection\" (DSMSCN)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        mid = out_ch // 3\n",
        "        remainder = out_ch - mid * 3\n",
        "\n",
        "        # Three parallel branches at different scales\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid, kernel_size=1),\n",
        "            nn.BatchNorm2d(mid),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid + remainder, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(mid + remainder),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Fusion: combine multi-scale features\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(mid * 3 + remainder, out_ch, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f1 = self.branch1x1(x)  # Fine details\n",
        "        f3 = self.branch3x3(x)  # Local context\n",
        "        f5 = self.branch5x5(x)  # Broader context\n",
        "        return self.fuse(torch.cat([f1, f3, f5], dim=1))\n",
        "\n",
        "\n",
        "# Quick test\n",
        "mfcu = MFCU(3, 32)\n",
        "test_input = torch.randn(1, 3, 64, 64)\n",
        "test_output = mfcu(test_input)\n",
        "print(f'MFCU: {test_input.shape} ‚Üí {test_output.shape}')"
      ],
      "metadata": {
        "id": "MlXZhdYxLAQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Siamese Encoder (Weight-Shared)\n",
        "\n",
        "The encoder processes each temporal image through 4 MFCU blocks with max-pooling. Both branches **share identical weights**, ensuring the images are mapped into the same feature space for meaningful comparison."
      ],
      "metadata": {
        "id": "4PR6C8EDLTyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Weight-shared encoder with 4 MFCU blocks.\n",
        "    Produces feature maps at 4 spatial scales.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc1 = MFCU(3, 32)\n",
        "        self.enc2 = MFCU(32, 64)\n",
        "        self.enc3 = MFCU(64, 128)\n",
        "        self.enc4 = MFCU(128, 256)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f1 = self.enc1(x)               # [B, 32,  H,   W]\n",
        "        f2 = self.enc2(self.pool(f1))    # [B, 64,  H/2, W/2]\n",
        "        f3 = self.enc3(self.pool(f2))    # [B, 128, H/4, W/4]\n",
        "        f4 = self.enc4(self.pool(f3))    # [B, 256, H/8, W/8]\n",
        "        return [f1, f2, f3, f4]\n",
        "\n",
        "\n",
        "# Verify feature map sizes\n",
        "enc = SiameseEncoder()\n",
        "test_img = torch.randn(1, 3, 256, 256)\n",
        "features = enc(test_img)\n",
        "for i, f in enumerate(features):\n",
        "    print(f'  Scale {i+1}: {f.shape}')"
      ],
      "metadata": {
        "id": "MfefU0UALRiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Change Detection Decoder (U-Net Style)\n",
        "\n",
        "The decoder takes **absolute-difference feature maps** from both branches at each scale and progressively upsamples them with skip connections. This recovers spatial resolution while preserving both fine and coarse change information."
      ],
      "metadata": {
        "id": "FC3rfw2LLYhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChangeDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net style decoder that processes multi-scale difference features.\n",
        "    Uses transposed convolutions for upsampling and skip connections.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Upsampling + MFCU blocks\n",
        "        self.up4 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec3 = MFCU(256, 128)   # 128 (upsampled) + 128 (skip)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec2 = MFCU(128, 64)    # 64 (upsampled) + 64 (skip)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec1 = MFCU(64, 32)     # 32 (upsampled) + 32 (skip)\n",
        "\n",
        "        # Final classification head\n",
        "        self.head = nn.Conv2d(32, 1, kernel_size=1)  # Binary output\n",
        "\n",
        "    def forward(self, diffs):\n",
        "        d1, d2, d3, d4 = diffs  # Multi-scale difference features\n",
        "\n",
        "        x = self.up4(d4)                            # [B, 128, H/4, W/4]\n",
        "        x = self.dec3(torch.cat([x, d3], dim=1))    # [B, 128, H/4, W/4]\n",
        "\n",
        "        x = self.up3(x)                              # [B, 64, H/2, W/2]\n",
        "        x = self.dec2(torch.cat([x, d2], dim=1))     # [B, 64, H/2, W/2]\n",
        "\n",
        "        x = self.up2(x)                              # [B, 32, H, W]\n",
        "        x = self.dec1(torch.cat([x, d1], dim=1))     # [B, 32, H, W]\n",
        "\n",
        "        return self.head(x)                           # [B, 1, H, W]"
      ],
      "metadata": {
        "id": "m1pQqCeKLVv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Complete Siamese Change Detection Network"
      ],
      "metadata": {
        "id": "JMWadmTULdvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiamMSCDNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Siamese Multi-Scale Change Detection Network.\n",
        "\n",
        "    Architecture:\n",
        "      1. Shared encoder processes both T1 and T2 images\n",
        "      2. Absolute difference computed at each feature scale\n",
        "      3. U-Net decoder produces pixel-wise change map\n",
        "\n",
        "    References:\n",
        "      - DSMSCN (Chen et al., 2019): Multi-scale feature convolution\n",
        "      - SNUNet-CD (Fang et al., 2021): Siamese nested architecture\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = SiameseEncoder()  # Shared weights\n",
        "        self.decoder = ChangeDecoder()\n",
        "\n",
        "    def forward(self, img_t1, img_t2):\n",
        "        # Extract features from both temporal images\n",
        "        feats_t1 = self.encoder(img_t1)  # [f1, f2, f3, f4]\n",
        "        feats_t2 = self.encoder(img_t2)  # Same encoder!\n",
        "\n",
        "        # Compute absolute difference at each scale\n",
        "        diffs = [torch.abs(f1 - f2)\n",
        "                 for f1, f2 in zip(feats_t1, feats_t2)]\n",
        "\n",
        "        # Decode differences into change map\n",
        "        logits = self.decoder(diffs)\n",
        "        return logits.squeeze(1)  # [B, H, W]\n",
        "\n",
        "\n",
        "# ‚îÄ‚îÄ Verify the full architecture ‚îÄ‚îÄ\n",
        "model = SiamMSCDNet()\n",
        "x1 = torch.randn(2, 3, 256, 256)\n",
        "x2 = torch.randn(2, 3, 256, 256)\n",
        "out = model(x1, x2)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Input shape:      2 √ó [B, 3, 256, 256]')\n",
        "print(f'Output shape:     {list(out.shape)}')\n",
        "print(f'Total parameters: {n_params:,}')\n",
        "print(f'Trainable params: {n_trainable:,}')\n",
        "print(f'Model size:       ~{n_params * 4 / 1e6:.1f} MB (float32)')"
      ],
      "metadata": {
        "id": "2LRehle-LacV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: #e3f2fd; border-left: 4px solid #1e88e5; padding: 12px; margin: 10px 0; border-radius: 4px;\">\n",
        "<b>üèóÔ∏è Architecture Summary:</b><br>\n",
        "<b>Input:</b> Two 256√ó256√ó3 RGB satellite images (T‚ÇÅ and T‚ÇÇ)<br>\n",
        "<b>Encoder:</b> 4 MFCU blocks with shared weights ‚Üí features at 4 scales<br>\n",
        "<b>Difference:</b> |F(T‚ÇÅ) ‚àí F(T‚ÇÇ)| computed at each scale<br>\n",
        "<b>Decoder:</b> U-Net upsampling with skip connections<br>\n",
        "<b>Output:</b> 256√ó256 binary change probability map\n",
        "</div>"
      ],
      "metadata": {
        "id": "EXHbFqksLioG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4. Part 3 ‚Äî Training the Model\n",
        "\n",
        "## 4.1 Loss Function: Dice + BCE\n",
        "\n",
        "Change detection is highly **imbalanced** (most pixels are unchanged). We combine:\n",
        "\n",
        "- **Binary Cross-Entropy (BCE)**: Standard pixel-wise classification loss\n",
        "- **Dice Loss**: Directly optimizes overlap between prediction and ground truth, helping with rare positive pixels"
      ],
      "metadata": {
        "id": "g4cN0-oWLk3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceBCELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Dice Loss + Binary Cross-Entropy Loss.\n",
        "    Dice Loss handles class imbalance by directly optimizing\n",
        "    the overlap metric (similar to F1 score).\n",
        "    \"\"\"\n",
        "    def __init__(self, bce_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def dice_loss(self, pred, target, smooth=1.0):\n",
        "        pred_sig = torch.sigmoid(pred)\n",
        "        intersection = (pred_sig * target).sum()\n",
        "        union = pred_sig.sum() + target.sum()\n",
        "        return 1 - (2.0 * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        bce = self.bce(pred, target)\n",
        "        dice = self.dice_loss(pred, target)\n",
        "        return self.bce_weight * bce + (1 - self.bce_weight) * dice"
      ],
      "metadata": {
        "id": "M0O684DtLfRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Evaluation Metrics\n",
        "\n",
        "High **Precision** ($\\frac{TP}{TP + FP}$) means the model is careful and doesn't produce many \"False Positives\" (crying wolf).\n",
        "\n",
        "High **Recall** ($\\frac{TP}{TP + FN}$) means the model is thorough and doesn't miss many \"False Negatives.\"\n",
        "\n",
        "**F1-Score** ($ 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$) penalizes extreme values; if either Precision or Recall is very low, the F1-score will be low.\n",
        "\n",
        "**IoU (Intersection over Union)** ($\\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$) measures the overlap between the predicted boundary and the ground truth boundary. IoU = 1: Perfect overlap. IoU > 0.5: Generally considered a \"good\" prediction in many benchmarks.\n",
        "\n",
        "**OA (Overall Accuracy)** ($\\frac{TP + TN}{\\text{Total Population}}$) It is the fraction of total predictions that were correct. OA can be very deceptive if your data is imbalanced. For example, if 99% of your data is \"Class A,\" a model that always predicts \"Class A\" will have a 99% OA but is actually useless for finding \"Class B.\""
      ],
      "metadata": {
        "id": "UzPIlAOWLohs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Evaluate change detection model.\n",
        "    Returns: dict with precision, recall, f1, iou, oa\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    TP, FP, FN, TN = 0, 0, 0, 0\n",
        "\n",
        "    for imgA, imgB, mask in loader:\n",
        "        imgA = imgA.to(device)\n",
        "        imgB = imgB.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        pred = torch.sigmoid(model(imgA, imgB)) > 0.5\n",
        "        pred_b = pred.bool()\n",
        "        mask_b = mask.bool()\n",
        "\n",
        "        TP += (pred_b & mask_b).sum().item()\n",
        "        FP += (pred_b & ~mask_b).sum().item()\n",
        "        FN += (~pred_b & mask_b).sum().item()\n",
        "        TN += (~pred_b & ~mask_b).sum().item()\n",
        "\n",
        "    eps = 1e-8\n",
        "    precision = TP / (TP + FP + eps)\n",
        "    recall    = TP / (TP + FN + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "    iou       = TP / (TP + FP + FN + eps)\n",
        "    oa        = (TP + TN) / (TP + FP + FN + TN + eps)\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'iou': iou,\n",
        "        'oa': oa\n",
        "    }"
      ],
      "metadata": {
        "id": "MRPWn170Lmn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Training Loop"
      ],
      "metadata": {
        "id": "Wu2mp_f1LtgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.cuda.amp as amp\n",
        "\n",
        "# 1. Set specific allocation config to prevent fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# 2. Hyperparameters\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# üßπ CLEANUP: Clear GPU before re-initializing\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- Data Loaders ---\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                          drop_last=True, persistent_workers=True)\n",
        "\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                          persistent_workers=True)\n",
        "\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
        "                          persistent_workers=True)\n",
        "\n",
        "# --- Model, Loss, Optimizer ---\n",
        "model = SiamMSCDNet().to(DEVICE)\n",
        "criterion = DiceBCELoss(bce_weight=0.5)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# 3. Initialize GradScaler for Mixed Precision\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "print(f'Training on {DEVICE} with {len(train_ds)} samples')\n",
        "print(f'Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}, LR: {LR}')\n",
        "\n",
        "history = {'train_loss': [], 'val_f1': [], 'val_iou': [], 'val_oa': [], 'lr': []}\n",
        "best_f1 = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # --- Train ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS}', leave=False)\n",
        "\n",
        "    for imgA, imgB, mask in pbar:\n",
        "        imgA, imgB, mask = imgA.to(DEVICE), imgB.to(DEVICE), mask.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with amp.autocast():\n",
        "            pred = model(imgA, imgB)\n",
        "            loss = criterion(pred, mask)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * imgA.size(0)\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Validate ---\n",
        "    # We use a custom evaluate function that strictly uses torch.no_grad()\n",
        "    metrics = evaluate(model, val_loader)\n",
        "    scheduler.step()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    # --- Log ---\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_f1'].append(metrics['f1'])\n",
        "    history['val_iou'].append(metrics['iou'])\n",
        "    history['val_oa'].append(metrics['oa'])\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    star = ''\n",
        "    if metrics['f1'] > best_f1:\n",
        "        best_f1 = metrics['f1']\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        star = ' ‚òÖ Best!'\n",
        "\n",
        "    print(f\"Epoch {epoch:3d}/{EPOCHS} ‚îÇ Loss: {train_loss:.4f} ‚îÇ \"\n",
        "          f\"F1: {metrics['f1']:.4f} ‚îÇ IoU: {metrics['iou']:.4f} ‚îÇ \"\n",
        "          f\"OA: {metrics['oa']:.4f} ‚îÇ LR: {current_lr:.6f}{star}\")\n",
        "\n",
        "print(f'\\n‚úÖ Training complete! Best validation F1: {best_f1:.4f}')"
      ],
      "metadata": {
        "id": "dmfvxgnzLqZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Curves"
      ],
      "metadata": {
        "id": "MoQBJ9wQLyeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], color='#A51C30', linewidth=2)\n",
        "axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Dice + BCE Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# F1 and IoU\n",
        "axes[1].plot(history['val_f1'], color='#1E5A96', linewidth=2, label='F1 Score')\n",
        "axes[1].plot(history['val_iou'], color='#4CAF50', linewidth=2, label='IoU')\n",
        "axes[1].set_title('Validation Metrics', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend(fontsize=12)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning Rate\n",
        "axes[2].plot(history['lr'], color='#FF9800', linewidth=2)\n",
        "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('LR')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qhk_fKPBLvSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5. Part 4 ‚Äî Evaluation and Visualization\n",
        "\n",
        "## 5.1 Test Set Evaluation"
      ],
      "metadata": {
        "id": "trIearzvL2lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate on test set\n",
        "model.load_state_dict(torch.load('best_model.pth', map_location=DEVICE))\n",
        "test_metrics = evaluate(model, test_loader)\n",
        "\n",
        "print('=' * 55)\n",
        "print('       TEST SET RESULTS')\n",
        "print('=' * 55)\n",
        "print(f'  Precision:  {test_metrics[\"precision\"]:.4f}')\n",
        "print(f'  Recall:     {test_metrics[\"recall\"]:.4f}')\n",
        "print(f'  F1 Score:   {test_metrics[\"f1\"]:.4f}')\n",
        "print(f'  IoU:        {test_metrics[\"iou\"]:.4f}')\n",
        "print(f'  Overall Acc:{test_metrics[\"oa\"]:.4f}')\n",
        "print('=' * 55)"
      ],
      "metadata": {
        "id": "TovySWeHL0Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Visualize Predictions"
      ],
      "metadata": {
        "id": "ePbeB9BCL6fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(model, dataset, indices, device=DEVICE):\n",
        "    \"\"\"Visualize model predictions alongside inputs and ground truth.\"\"\"\n",
        "    model.eval()\n",
        "    n = len(indices)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std  = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    fig, axes = plt.subplots(n, 4, figsize=(18, 4.5 * n))\n",
        "    if n == 1: axes = axes[np.newaxis, :]\n",
        "\n",
        "    col_titles = ['Time 1 (Before)', 'Time 2 (After)', 'Ground Truth', 'Prediction']\n",
        "    for j, t in enumerate(col_titles):\n",
        "        axes[0, j].set_title(t, fontsize=13, fontweight='bold')\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        imgA, imgB, mask = dataset[idx]\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(\n",
        "                model(imgA.unsqueeze(0).to(device),\n",
        "                      imgB.unsqueeze(0).to(device))).cpu().squeeze()\n",
        "        pred_mask = (pred > 0.5).float().numpy()\n",
        "\n",
        "        imgA_np = np.clip(imgA.permute(1,2,0).numpy() * std + mean, 0, 1)\n",
        "        imgB_np = np.clip(imgB.permute(1,2,0).numpy() * std + mean, 0, 1)\n",
        "\n",
        "        axes[i, 0].imshow(imgA_np)\n",
        "        axes[i, 1].imshow(imgB_np)\n",
        "        axes[i, 2].imshow(mask.numpy(), cmap='hot', vmin=0, vmax=1)\n",
        "        axes[i, 3].imshow(pred_mask, cmap='hot', vmin=0, vmax=1)\n",
        "\n",
        "        for ax in axes[i]: ax.axis('off')\n",
        "\n",
        "    plt.suptitle('Model Predictions on Test Set', fontsize=16, fontweight='bold', y=1.01)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('predictions.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Show 6 test predictions\n",
        "np.random.seed(123)\n",
        "test_indices = np.random.choice(len(test_ds), 6, replace=False)\n",
        "visualize_predictions(model, test_ds, test_indices)"
      ],
      "metadata": {
        "id": "9TVT0jueL4ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Error Analysis: Confusion Maps\n",
        "\n",
        "Color-coded confusion maps reveal where the model succeeds and fails:\n",
        "- üü¢ **Green** = True Positive (correctly detected change)\n",
        "- üî¥ **Red** = False Positive (false alarm)\n",
        "- üîµ **Blue** = False Negative (missed change)"
      ],
      "metadata": {
        "id": "DQqijaXKMJ-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_maps(model, dataset, indices, device=DEVICE):\n",
        "    \"\"\"Create color-coded confusion maps for error analysis.\"\"\"\n",
        "    model.eval()\n",
        "    n = len(indices)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std  = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    fig, axes = plt.subplots(n, 3, figsize=(15, 5 * n))\n",
        "    if n == 1: axes = axes[np.newaxis, :]\n",
        "\n",
        "    col_titles = ['Time 2 (After)', 'Ground Truth', 'Confusion Map']\n",
        "    for j, t in enumerate(col_titles):\n",
        "        axes[0, j].set_title(t, fontsize=13, fontweight='bold')\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        imgA, imgB, mask = dataset[idx]\n",
        "        with torch.no_grad():\n",
        "            pred = (torch.sigmoid(\n",
        "                model(imgA.unsqueeze(0).to(device),\n",
        "                      imgB.unsqueeze(0).to(device))).cpu().squeeze() > 0.5)\n",
        "\n",
        "        mask_b = mask.bool()\n",
        "        pred_b = pred.bool()\n",
        "\n",
        "        # Build confusion map\n",
        "        h, w = mask.shape\n",
        "        confusion = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "        confusion[mask_b & pred_b]   = [0, 200, 0]      # TP: Green\n",
        "        confusion[~mask_b & pred_b]  = [220, 50, 50]     # FP: Red\n",
        "        confusion[mask_b & ~pred_b]  = [50, 80, 220]     # FN: Blue\n",
        "\n",
        "        imgB_np = np.clip(imgB.permute(1,2,0).numpy() * std + mean, 0, 1)\n",
        "\n",
        "        axes[i, 0].imshow(imgB_np)\n",
        "        axes[i, 1].imshow(mask.numpy(), cmap='hot', vmin=0, vmax=1)\n",
        "        axes[i, 2].imshow(confusion)\n",
        "\n",
        "        # Count stats\n",
        "        tp = (mask_b & pred_b).sum().item()\n",
        "        fp = (~mask_b & pred_b).sum().item()\n",
        "        fn = (mask_b & ~pred_b).sum().item()\n",
        "        axes[i, 2].text(5, 15, f'TP:{tp} FP:{fp} FN:{fn}',\n",
        "                        color='white', fontsize=9, fontweight='bold',\n",
        "                        bbox=dict(facecolor='black', alpha=0.7))\n",
        "\n",
        "        for ax in axes[i]: ax.axis('off')\n",
        "\n",
        "    # Legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='green', label='True Positive'),\n",
        "        Patch(facecolor='red', label='False Positive'),\n",
        "        Patch(facecolor='blue', label='False Negative'),\n",
        "    ]\n",
        "    fig.legend(handles=legend_elements, loc='lower center',\n",
        "              ncol=3, fontsize=12, bbox_to_anchor=(0.5, -0.02))\n",
        "    plt.suptitle('Error Analysis: Confusion Maps', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_maps.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_confusion_maps(model, test_ds, test_indices[:4])"
      ],
      "metadata": {
        "id": "BJX9ZTlvL8XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 6. Part 5 ‚Äî Experiments and Extensions\n",
        "\n",
        "## Transfer to Damage Detection\n",
        "\n",
        "The [xBD dataset](https://xview2.org/) (from the xView2 challenge) provides satellite imagery of disaster-affected areas with building damage labels at four severity levels. A model pre-trained on LEVIR-CD for building change can be **fine-tuned** for damage classification.\n",
        "\n",
        "The idea: your LEVIR-CD encoder already knows how to detect *structural change* in buildings. Damage (from earthquakes, hurricanes, etc.) is fundamentally a type of structural change. By freezing the encoder and training a new classification head, you can transfer these learned features.\n",
        "\n",
        "```python\n",
        "# Pseudocode for transfer learning to damage detection\n",
        "# 1. Load pre-trained encoder weights\n",
        "pretrained = SiamMSCDNet()\n",
        "pretrained.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# 2. Freeze encoder\n",
        "for param in pretrained.encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 3. Replace decoder head for 4-class damage grading\n",
        "# (no damage, minor, major, destroyed)\n",
        "pretrained.decoder.head = nn.Conv2d(32, 4, kernel_size=1)\n",
        "\n",
        "# 4. Fine-tune on xBD with CrossEntropyLoss\n",
        "```"
      ],
      "metadata": {
        "id": "8gS6REkqMOFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 7. Discussion Questions\n",
        "\n",
        "Answer these questions in markdown cells below (1‚Äì2 paragraphs each).\n",
        "\n",
        "**Q1.** Why do Siamese networks use weight sharing between the two branches? What would happen if each branch had independent weights? What are the implications for the learned feature space?\n",
        "\n",
        "**Q2.** Examine your confusion maps. What types of errors does the model make most frequently? Are false positives or false negatives more common, and why might that be?\n",
        "\n",
        "**Q3.** How does the class imbalance between changed and unchanged pixels affect training? How does the Dice Loss component help, compared to using BCE alone?\n",
        "\n",
        "**Q4.** Compare the multi-scale (MFCU) vs. single-scale ablation results. Why would multi-scale features improve change detection for buildings of varying sizes?\n",
        "\n",
        "**Q5.** How could this change detection approach be applied to post-earthquake damage assessment? What additional challenges would arise compared to the LEVIR-CD scenario?\n",
        "\n",
        "**Q6.** What role does the temporal gap between images play in change detection accuracy? How might very short or very long time gaps affect model performance?"
      ],
      "metadata": {
        "id": "N4tebIsNMhJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 Answer:**\n",
        "\n",
        "*(Your answer here)*\n",
        "\n",
        "**Q2 Answer:**\n",
        "\n",
        "*(Your answer here)*\n",
        "\n",
        "**Q3 Answer:**\n",
        "\n",
        "*(Your answer here)*\n",
        "\n",
        "**Q4 Answer:**\n",
        "\n",
        "*(Your answer here)*\n",
        "\n",
        "**Q5 Answer:**\n",
        "\n",
        "*(Your answer here)*\n",
        "\n",
        "**Q6 Answer:**\n",
        "\n",
        "*(Your answer here)*"
      ],
      "metadata": {
        "id": "8gLkyb88Mj4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 8. References and Resources\n",
        "\n",
        "### Key Papers\n",
        "\n",
        "1. Chen, H., Wu, C., Du, B., & Zhang, L. (2019). *Deep Siamese Multi-scale Convolutional Network for Change Detection in Multi-Temporal VHR Images.* MultiTemp 2019. [[Code]](https://github.com/ChenHongruixuan/DSMSCN)\n",
        "2. Chen, H. & Shi, Z. (2020). *A Spatial-Temporal Attention-Based Method and a New Dataset for Remote Sensing Image Change Detection.* Remote Sensing, 12(10). [[LEVIR-CD]](https://justchenhao.github.io/LEVIR/)\n",
        "3. Fang, S. et al. (2021). *SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images.* IEEE GRSL. [[Code]](https://github.com/likyoo/Siam-NestedUNet)\n",
        "4. Chen, H. et al. (2022). *Unsupervised Change Detection in Multitemporal VHR Images Based on Deep Kernel PCA Convolutional Mapping Network.* IEEE TCYB. [[Code]](https://github.com/ChenHongruixuan/KPCAMNet)\n",
        "5. Zhang, M. et al. (2020). *Change Detection Based on Artificial Intelligence: State-of-the-Art and Challenges.* [[Review]](https://github.com/MinZHANG-WHU/Change-Detection-Review)\n",
        "\n",
        "### Code Repositories\n",
        "\n",
        "| Repository | Description |\n",
        "|---|---|\n",
        "| [DSMSCN](https://github.com/ChenHongruixuan/DSMSCN) | TensorFlow implementation of Siamese Multi-Scale CNN |\n",
        "| [Siam-NestedUNet](https://github.com/likyoo/Siam-NestedUNet) | PyTorch Siamese Nested U-Net (SNUNet-CD) |\n",
        "| [KPCAMNet](https://github.com/ChenHongruixuan/KPCAMNet) | Unsupervised CD with deep kernel PCA |\n",
        "| [ChangeDetectionRepository](https://github.com/ChenHongruixuan/ChangeDetectionRepository) | Traditional & DL-based CD methods collection |\n",
        "| [Change-Detection-Review](https://github.com/MinZHANG-WHU/Change-Detection-Review) | Comprehensive review with code & datasets |\n",
        "| [Awesome RS CD](https://github.com/wenhwu/awesome-remote-sensing-change-detection) | Datasets, methods, competitions |\n",
        "\n",
        "### Datasets\n",
        "\n",
        "| Dataset | Description |\n",
        "|---|---|\n",
        "| [LEVIR-CD](https://justchenhao.github.io/LEVIR/) | 637 VHR image pairs, 31K building changes, Texas |\n",
        "| [WHU Building CD](http://sigma.whu.edu.cn/resource.php) | Christchurch, NZ (2012 earthquake) |\n",
        "| [SZTAKI AirChange](http://mplab.sztaki.hu/remotesensing/airchange_benchmark.html) | Multi-temporal aerial change benchmark |\n",
        "| [xBD / xView2](https://xview2.org/) | Building damage assessment across disasters |"
      ],
      "metadata": {
        "id": "CD8eHO0KMl7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "# üî¥ Activity 2: Martian Crater Detection with YOLO\n",
        "### Using the [2022 GeoAI Martian Challenge](http://cici.lab.asu.edu/martian/#home) Dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. Work with a **planetary science benchmark dataset**\n",
        "2. Train (fine tune) a **YOLO** object detection model to locate Martian craters\n",
        "3. Evaluate detection performance using standard metrics (precision, recall, mAP)\n",
        "4. Analyze model behavior across different crater sizes and terrain types\n",
        "\n",
        "---\n",
        "\n",
        "## Background\n",
        "\n",
        "### Why Martian Crater Detection?\n",
        "\n",
        "Impact craters are the dominant landform on Mars. A global census of craters enables:\n",
        "\n",
        "- **Surface age dating** ‚Äî Crater size-frequency distributions are the primary chronometer for planetary surfaces.\n",
        "- **Geological mapping** ‚Äî Crater morphology reveals subsurface ice, lava flows, and erosion history.\n",
        "- **Landing site hazard assessment** ‚Äî Missions like Perseverance and future crewed landings need automated terrain analysis.\n",
        "- **Climate history** ‚Äî Crater degradation patterns record billions of years of atmospheric and fluvial erosion.\n",
        "\n",
        "### The 2022 GeoAI Martian Challenge Dataset\n",
        "\n",
        "This benchmark dataset was developed by the [ASU CICI Lab](https://cici.lab.asu.edu/martian/) ([Hsu et al., 2021](https://www.mdpi.com/2072-4292/13/11/2116)) and assembles:\n",
        "\n",
        "- **102,675 images** extracted from a global Mars mosaic\n",
        "- **301,912 annotated craters** with bounding boxes\n",
        "- **Source imagery:** Mars Odyssey THEMIS (Thermal Emission Imaging System) daytime infrared, 100 m/pixel ([Edwards et al., 2011](https://doi.org/10.1029/2010JE003755))\n",
        "- **Crater labels:** From the [Robbins & Hynek (2012)](https://doi.org/10.1029/2011JE003966) global catalog of 640K+ craters\n",
        "- **Image size:** 256√ó256 pixels (25.6√ó25.6 km per tile)\n",
        "- **Crater sizes:** 0.2 km (2 px) to 25.5 km (255 px) in diameter\n",
        "\n",
        "The dataset was designed for a formal AI competition, with train/val/test splits and COCO-format annotations.\n",
        "\n",
        "### Crater Size Groups\n",
        "\n",
        "| Group | Diameter | Pixels | Count | % |\n",
        "|-------|----------|--------|-------|---|\n",
        "| Small | 0.2‚Äì1 km | 2‚Äì10 px | 115,871 | 38% |\n",
        "| Medium | 1.1‚Äì5 km | 11‚Äì50 px | 172,251 | 57% |\n",
        "| Large | 5‚Äì25.5 km | 50‚Äì255 px | 13,790 | 5% |\n",
        "\n",
        "### YOLO: You Only Look Once\n",
        "\n",
        "YOLO is a family of **single-stage object detectors** that predict bounding boxes and class probabilities in one forward pass:\n",
        "\n",
        "```\n",
        "Image ‚Üí Backbone (features) ‚Üí Neck (multi-scale fusion) ‚Üí Head ‚Üí Boxes + Confidence\n",
        "```\n",
        "\n",
        "Key concepts: bounding boxes as `(x_center, y_center, width, height)`, confidence thresholds, Non-Maximum Suppression (NMS), and mean Average Precision (mAP).\n",
        "\n",
        "> ‚ö†Ô∏è **Note on dataset size:** The full dataset is 4.3 GB. For this 1-hour lab, we use a **subset** (~5,000 training images) so training completes in ~10 min on a T4 GPU. The full dataset can be used for research projects.\n"
      ],
      "metadata": {
        "id": "jNXZp-7so4fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. Background\n",
        "\n",
        "## 1.1 Semantic Segmentation vs. Object Detection\n",
        "\n",
        "Two fundamental computer vision tasks in remote sensing are:\n",
        "\n",
        "- **Semantic segmentation**: Classify *every pixel* into categories (e.g., crater rim vs. background). The output is a mask the same size as the input image.\n",
        "- **Object detection**: Locate objects with *bounding boxes* and classify them (e.g., draw a box around each ship in a SAR image). The output is a list of (x, y, w, h, class) tuples.\n",
        "\n",
        "## 1.2 The DeepMoon Approach\n",
        "\n",
        "The [DeepMoon](https://github.com/silburt/DeepMoon) project (Silburt et al., 2019) demonstrated that a CNN can identify lunar craters from Digital Elevation Maps (DEMs) with high accuracy. Their pipeline:\n",
        "\n",
        "1. **Input**: DEM image patches of the lunar surface (grayscale elevation data)\n",
        "2. **Target**: Binary ring masks where crater rims are marked as white annuli\n",
        "3. **Model**: A U-Net-style encoder‚Äìdecoder network that outputs a pixel-wise probability map\n",
        "4. **Post-processing**: Template matching on the predicted ring mask to extract (x, y, radius) for each crater\n",
        "\n",
        "The key insight is that craters appear as circular depressions in elevation data, and their rims produce characteristic ring-shaped gradients that CNNs can learn to detect.\n",
        "\n",
        "## 1.3 SAR Ship Detection (YOLO-style)\n",
        "\n",
        "The [SAR_yolov3](https://github.com/humblecoder612/SAR_yolov3) project applies the YOLO (You Only Look Once) object detection framework to Synthetic Aperture Radar (SAR) satellite imagery for ship detection. SAR sensors are immune to weather and lighting conditions, making them ideal for maritime surveillance. YOLO-style detectors process the entire image in a single pass and output bounding boxes directly, achieving real-time performance. In Part 5, we extend our U-Net framework toward a simple detection head."
      ],
      "metadata": {
        "id": "INNQRsWcpDms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Part 1: Setup & Installation (~3 min)\n",
        "!pip install -q ultralytics pycocotools\n",
        "\n",
        "import os, json, glob, random, shutil, yaml\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}  |  CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "Uqh0g1-8o6L8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 2: Download & Explore the Dataset (~10 min)\n",
        "\n",
        "We download the 2022 GeoAI Martian Challenge Dataset directly from ASU's server."
      ],
      "metadata": {
        "id": "7eGWhR7WqqvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Download, Unzip & Organize Martian Dataset { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import gdown\n",
        "import glob\n",
        "\n",
        "DATA_DIR = '/content/MARTIAN_DATASET'\n",
        "ZIP_PATH = os.path.join(DATA_DIR, \"martian_yolo_dataset.zip\")\n",
        "EXTRACT_DIR = os.path.join(DATA_DIR, \"extracted\")\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "print(\"üöÄ Starting download from Google Drive ...\")\n",
        "\n",
        "file_ids = {\n",
        "    'martian_yolo_dataset.zip': '1CIIqqVVdfnyF7EV7X27fhkvPvhbrVSv_',\n",
        "}\n",
        "\n",
        "for filename, file_id in file_ids.items():\n",
        "    output_path = os.path.join(DATA_DIR, filename)\n",
        "    if not os.path.exists(output_path):\n",
        "        print(f\"   Downloading {filename}...\")\n",
        "        gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "    else:\n",
        "        print(f\"   {filename} already exists, skipping download.\")\n",
        "\n",
        "# Extract\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    print(\"Extracting dataset...\")\n",
        "    !unzip -q \"{ZIP_PATH}\" -d \"{EXTRACT_DIR}\"\n",
        "else:\n",
        "    print(\"Already extracted.\")\n",
        "\n",
        "# Find the root of the extracted data\n",
        "# It may be nested inside a subfolder\n",
        "candidates = glob.glob(os.path.join(EXTRACT_DIR, '**', 'ids.json'), recursive=True)\n",
        "if candidates:\n",
        "    CHALLENGE_DIR = os.path.dirname(candidates[0])\n",
        "else:\n",
        "    # Try direct path\n",
        "    CHALLENGE_DIR = EXTRACT_DIR\n",
        "\n",
        "print(f\"Dataset root: {CHALLENGE_DIR}\")\n",
        "print(\"Contents:\")\n",
        "if os.path.exists(CHALLENGE_DIR):\n",
        "    for item in sorted(os.listdir(CHALLENGE_DIR)):\n",
        "        full = os.path.join(CHALLENGE_DIR, item)\n",
        "        if os.path.isdir(full):\n",
        "            n_files = len(os.listdir(full))\n",
        "            print(f\"  üìÅ {item}/ ({n_files:,} files)\")\n",
        "        else:\n",
        "            print(f\"  üìÑ {item} ({os.path.getsize(full)/1e6:.1f} MB)\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Directory {CHALLENGE_DIR} does not exist. Extraction might have failed.\")\n"
      ],
      "metadata": {
        "id": "BroovxoleNLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def print_yolo_stats(base_dir):\n",
        "    print(f\"Dataset Statistics for: {base_dir}\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"{'Split':<10} | {'Images':<8} | {'Craters':<8} | {'Avg Craters/Img':<15}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        img_dir = os.path.join(base_dir, split, 'images')\n",
        "        lbl_dir = os.path.join(base_dir, split, 'labels')\n",
        "\n",
        "        if not os.path.exists(img_dir):\n",
        "            print(f\"{split:<10} | Directory not found\")\n",
        "            continue\n",
        "\n",
        "        images = [f for f in os.listdir(img_dir) if f.endswith(('.png', '.jpg'))]\n",
        "        labels = [f for f in os.listdir(lbl_dir) if f.endswith('.txt')]\n",
        "\n",
        "        total_craters = 0\n",
        "        for lbl in labels:\n",
        "            with open(os.path.join(lbl_dir, lbl), 'r') as f:\n",
        "                total_craters += len(f.readlines())\n",
        "\n",
        "        n_img = len(images)\n",
        "        avg = total_craters / n_img if n_img > 0 else 0\n",
        "\n",
        "        print(f\"{split:<10} | {n_img:<8,} | {total_craters:<8,} | {avg:<15.2f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Run stats for the Martian YOLO folder\n",
        "print_yolo_stats('/content/martian_yolo')\n"
      ],
      "metadata": {
        "id": "EwpQ-6uEf7M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚ùì Questions to consider:**\n",
        "\n",
        "1. Most craters are very small (< 10 pixels). Why is small-object detection particularly challenging for neural networks?\n",
        "2. The images are THEMIS daytime **infrared** rather than optical. What surface properties does IR capture that visible light does not?\n",
        "3. Some images have many craters, others just one. How might this imbalance affect training?"
      ],
      "metadata": {
        "id": "9QRccVimX0_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write data.yaml for YOLO\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# Ensure YOLO_DIR is defined\n",
        "YOLO_DIR = \"/content/martian_yolo\"\n",
        "\n",
        "data_yaml = {\n",
        "    'path': YOLO_DIR,\n",
        "    'train': 'train/images',\n",
        "    'val': 'val/images',\n",
        "    'test': 'test/images',\n",
        "    'nc': 1,\n",
        "    'names': ['crater'],\n",
        "}\n",
        "\n",
        "yaml_path = os.path.join(\"/content/MARTIAN_DATASET/extracted\", 'data.yaml')\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Created config at: {yaml_path}\")\n",
        "print(open(yaml_path).read())"
      ],
      "metadata": {
        "id": "Tk4qvFtTX-YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify: visualize YOLO-format annotations on sample tiles\n",
        "import matplotlib.patches as patches\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def show_yolo_samples(yolo_dir, split='train', n=6, seed=42):\n",
        "    img_dir = os.path.join(yolo_dir, split, 'images')\n",
        "    lbl_dir = os.path.join(yolo_dir, split, 'labels')\n",
        "\n",
        "    all_imgs = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
        "    # Prefer images with annotations\n",
        "    with_labels = [p for p in all_imgs\n",
        "                   if os.path.getsize(os.path.join(lbl_dir, Path(p).stem + '.txt')) > 0]\n",
        "    random.seed(seed)\n",
        "    samples = random.sample(with_labels, min(n, len(with_labels)))\n",
        "\n",
        "    cols = 3\n",
        "    rows = (n + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5.5 * rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, img_path in enumerate(samples):\n",
        "        img = np.array(Image.open(img_path))\n",
        "        h, w = img.shape[:2]\n",
        "        axes[idx].imshow(img, cmap='gray')\n",
        "\n",
        "        lbl_path = os.path.join(lbl_dir, Path(img_path).stem + '.txt')\n",
        "        n_cr = 0\n",
        "        with open(lbl_path) as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    _, xc, yc, bw, bh = [float(v) for v in parts]\n",
        "                    x1 = (xc - bw/2) * w\n",
        "                    y1 = (yc - bh/2) * h\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x1, y1), bw*w, bh*h,\n",
        "                        linewidth=1.2, edgecolor='cyan', facecolor='none'\n",
        "                    )\n",
        "                    axes[idx].add_patch(rect)\n",
        "                    n_cr += 1\n",
        "\n",
        "        axes[idx].set_title(Path(img_path).stem, fontsize=9)\n",
        "        axes[idx].set_xlabel(f'{n_cr} craters', fontsize=9)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    for idx in range(len(samples), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle(f'YOLO-Format Annotations ‚Äî {split} set',\n",
        "                fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_yolo_samples(YOLO_DIR, 'train', n=6)"
      ],
      "metadata": {
        "id": "iJYJYKa9YAhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚ùì Questions:**\n",
        "\n",
        "1. Why did we include ~10% **negative images** (no craters) in the training set?\n",
        "2. The COCO‚ÜíYOLO conversion normalizes coordinates to [0,1]. Why is this beneficial for training?\n",
        "3. Some craters span just 2-3 pixels. Should we filter these out, or keep them? What are the tradeoffs?"
      ],
      "metadata": {
        "id": "SF8_Wq6iYGjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 4: Train YOLOv8 for Crater Detection (~20 min)\n",
        "\n",
        "We use **YOLOv8n** (nano, 3.2M params) for fast training. The model is pretrained on COCO (everyday objects) and we fine-tune it on Martian craters ‚Äî a compelling case of **transfer learning** across domains."
      ],
      "metadata": {
        "id": "tDvUpZeHYJP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov5n.pt')  # try the latest pretrained model: yolov8n.pt\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "yAYJyDT2YCJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ‚Äî expect ~20 min on a T4 GPU with our subset\n",
        "results = model.train(\n",
        "    data=yaml_path,\n",
        "    epochs=25,               # Adjust based on time\n",
        "    imgsz=256,               # Native image size\n",
        "    batch=64,                # Increase if GPU allows\n",
        "    name='martian_craters',\n",
        "    patience=10,             # Early stopping\n",
        "    save=True,\n",
        "    plots=True,\n",
        "    # Augmentation\n",
        "    flipud=0.5,              # Craters look the same flipped\n",
        "    fliplr=0.5,\n",
        "    degrees=180.0,           # Full rotation (craters are rotationally symmetric)\n",
        "    mosaic=1.0,              # Mosaic: stitch 4 images into one\n",
        "    mixup=0.1,               # MixUp: blend two images\n",
        "    hsv_h=0.0,               # No hue shift (grayscale IR)\n",
        "    hsv_s=0.0,               # No saturation shift\n",
        "    hsv_v=0.3,               # Brightness variation (simulates different thermal conditions)\n",
        "    scale=0.3,               # Scale augmentation for multi-size craters\n",
        ")"
      ],
      "metadata": {
        "id": "IpV6cFgrYLb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚è±Ô∏è While the model trains**, let's think about what's happening:\n",
        "\n",
        "1. How YOLOv5 differe from earlier versions and YOLOv8?\n",
        "2. **Loss components**:\n",
        "   - **CIoU box loss**: Penalizes incorrect box placement (center, size, aspect ratio)\n",
        "   - **BCE classification loss**: Binary cross-entropy for crater vs. background\n",
        "   - **DFL (Distribution Focal Loss)**: Refined bounding box regression"
      ],
      "metadata": {
        "id": "dkXLcV3VYROX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 5: Evaluate & Visualize Results (~15 min)"
      ],
      "metadata": {
        "id": "Xq837h_7YUnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image as IPImage\n",
        "\n",
        "train_dir = Path(results.save_dir)\n",
        "print(f\"Results saved to: {train_dir}\")\n",
        "\n",
        "# Training curves\n",
        "for fname in ['results.png']:\n",
        "    fpath = train_dir / fname\n",
        "    if fpath.exists():\n",
        "        display(IPImage(filename=str(fpath), width=900))"
      ],
      "metadata": {
        "id": "bCnhB-Q6YNgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block performs a formal evaluation of your best-trained model.\n",
        "\n",
        "**Loading the Best Model**: It initializes a new YOLO object using best.pt, which is the version of the model that achieved the highest performance during training.\n",
        "\n",
        "**Looping through Splits**: It iterates through both the 'val' (validation) and 'test' datasets.\n",
        "\n",
        "**Running Validation**: The model.val() function runs the model on those specific images and calculates standard object detection metrics.\n",
        "\n",
        "**Printing Metrics**: Finally, it prints key performance indicators:\n",
        "\n",
        "  * Precision: How many of the detected craters were actually real craters.\n",
        "\n",
        "  * Recall: What percentage of all real craters the model successfully found.\n",
        "  \n",
        "  * mAP@50: The Mean Average Precision at an Intersection over Union (IoU) threshold of 0.5 (a standard accuracy measure).\n",
        "  \n",
        "  * mAP@50-95: A more rigorous metric that averages precision across multiple IoU thresholds, reflecting how perfectly the bounding boxes fit the craters."
      ],
      "metadata": {
        "id": "Nkr57D2An8Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formal evaluation\n",
        "best_model = YOLO(str(train_dir / 'weights' / 'best.pt'))\n",
        "\n",
        "for split_name in ['val']:\n",
        "    metrics = best_model.val(data=yaml_path, imgsz=256, split=split_name, verbose=False)\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{split_name.upper()} SET METRICS\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"  Precision:   {metrics.box.mp:.3f}\")\n",
        "    print(f\"  Recall:      {metrics.box.mr:.3f}\")\n",
        "    print(f\"  mAP@50:      {metrics.box.map50:.3f}\")\n",
        "    print(f\"  mAP@50-95:   {metrics.box.map:.3f}\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "id": "hrsPJ9KCYXyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize detections vs. ground truth\n",
        "def visualize_detections(model, yolo_dir, split='val', n=6, conf=0.25, seed=99):\n",
        "    img_dir = os.path.join(yolo_dir, split, 'images')\n",
        "    lbl_dir = os.path.join(yolo_dir, split, 'labels')\n",
        "\n",
        "    all_imgs = sorted(glob.glob(os.path.join(img_dir, '*.png')))\n",
        "    with_labels = [p for p in all_imgs\n",
        "                   if os.path.getsize(os.path.join(lbl_dir, Path(p).stem + '.txt')) > 0]\n",
        "    random.seed(seed)\n",
        "    samples = random.sample(with_labels, min(n, len(with_labels)))\n",
        "\n",
        "    cols = 3\n",
        "    rows = (n + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(16, 5.5 * rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, img_path in enumerate(samples):\n",
        "        img = np.array(Image.open(img_path))\n",
        "        h, w = img.shape[:2]\n",
        "        axes[idx].imshow(img, cmap='gray')\n",
        "\n",
        "        # Ground truth (green dashed)\n",
        "        lbl_path = os.path.join(lbl_dir, Path(img_path).stem + '.txt')\n",
        "        n_gt = 0\n",
        "        with open(lbl_path) as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    _, xc, yc, bw, bh = [float(v) for v in parts]\n",
        "                    rect = patches.Rectangle(\n",
        "                        ((xc-bw/2)*w, (yc-bh/2)*h), bw*w, bh*h,\n",
        "                        linewidth=1.5, edgecolor='lime', facecolor='none', linestyle='--'\n",
        "                    )\n",
        "                    axes[idx].add_patch(rect)\n",
        "                    n_gt += 1\n",
        "\n",
        "        # Predictions (red solid)\n",
        "        preds = model.predict(img_path, conf=conf, verbose=False)\n",
        "        n_pred = 0\n",
        "        if preds[0].boxes is not None:\n",
        "            for box in preds[0].boxes:\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                c = box.conf[0].cpu().numpy()\n",
        "                rect = patches.Rectangle(\n",
        "                    (x1, y1), x2-x1, y2-y1,\n",
        "                    linewidth=1.5, edgecolor='red', facecolor='none'\n",
        "                )\n",
        "                axes[idx].add_patch(rect)\n",
        "                axes[idx].text(x1, max(y1-3, 8), f'{c:.2f}', color='red', fontsize=7,\n",
        "                              fontweight='bold',\n",
        "                              bbox=dict(boxstyle='round,pad=0.1', facecolor='black', alpha=0.5))\n",
        "                n_pred += 1\n",
        "\n",
        "        axes[idx].set_title(Path(img_path).stem, fontsize=9)\n",
        "        axes[idx].set_xlabel(f'GT: {n_gt} (green) | Pred: {n_pred} (red)', fontsize=9)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    for idx in range(len(samples), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend = [Line2D([0],[0], color='lime', ls='--', lw=2, label='Ground Truth'),\n",
        "              Line2D([0],[0], color='red', lw=2, label='Prediction')]\n",
        "    fig.legend(handles=legend, loc='upper center', ncol=2, fontsize=11,\n",
        "              bbox_to_anchor=(0.5, 1.02))\n",
        "    plt.suptitle(f'Crater Detection ‚Äî {split} (conf ‚â• {conf})',\n",
        "                fontsize=14, fontweight='bold', y=1.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_detections(best_model, YOLO_DIR, 'val', n=6)"
      ],
      "metadata": {
        "id": "lIJYAmdWYZVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze performance by crater size\n",
        "# Bin detections into Small / Medium / Large and measure recall for each\n",
        "print(\"Analyzing detection performance by crater size...\\n\")\n",
        "\n",
        "size_bins = {'Small (‚â§10 px)': (0, 10), 'Medium (11-50 px)': (11, 50), 'Large (>50 px)': (51, 999)}\n",
        "size_stats = {k: {'total': 0, 'detected': 0} for k in size_bins}\n",
        "\n",
        "val_imgs = sorted(glob.glob(os.path.join(YOLO_DIR, 'val', 'images', '*.png')))\n",
        "for img_path in val_imgs[:200]:  # Analyze first 200 val images\n",
        "    lbl_path = os.path.join(YOLO_DIR, 'val', 'labels', Path(img_path).stem + '.txt')\n",
        "    if not os.path.exists(lbl_path) or os.path.getsize(lbl_path) == 0:\n",
        "        continue\n",
        "\n",
        "    gt_boxes = []\n",
        "    with open(lbl_path) as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 5:\n",
        "                _, xc, yc, bw, bh = [float(v) for v in parts]\n",
        "                gt_boxes.append((xc, yc, bw, bh))\n",
        "\n",
        "    preds = best_model.predict(img_path, conf=0.25, verbose=False)\n",
        "    pred_boxes = []\n",
        "    if preds[0].boxes is not None:\n",
        "        for box in preds[0].boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            pred_boxes.append(((x1+x2)/2/256, (y1+y2)/2/256, (x2-x1)/256, (y2-y1)/256))\n",
        "\n",
        "    for gxc, gyc, gbw, gbh in gt_boxes:\n",
        "        diam_px = max(gbw, gbh) * 256\n",
        "        for label, (lo, hi) in size_bins.items():\n",
        "            if lo <= diam_px <= hi:\n",
        "                size_stats[label]['total'] += 1\n",
        "                # Check if any prediction overlaps (simple center-distance match)\n",
        "                for pxc, pyc, pbw, pbh in pred_boxes:\n",
        "                    dist = ((gxc - pxc)**2 + (gyc - pyc)**2)**0.5\n",
        "                    if dist < max(gbw, gbh) * 0.5:\n",
        "                        size_stats[label]['detected'] += 1\n",
        "                        break\n",
        "                break\n",
        "\n",
        "print(f\"{'Size Group':<20} {'Total':>8} {'Detected':>10} {'Recall':>8}\")\n",
        "print(\"-\" * 48)\n",
        "for label, s in size_stats.items():\n",
        "    recall = s['detected'] / max(s['total'], 1)\n",
        "    print(f\"{label:<20} {s['total']:>8} {s['detected']:>10} {recall:>8.1%}\")"
      ],
      "metadata": {
        "id": "jNPylSonYbUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence threshold analysis\n",
        "sample_img = sorted(glob.glob(os.path.join(YOLO_DIR, 'val', 'images', '*.png')))\n",
        "# Pick an image with several craters\n",
        "for p in sample_img:\n",
        "    lp = os.path.join(YOLO_DIR, 'val', 'labels', Path(p).stem + '.txt')\n",
        "    if os.path.exists(lp) and os.path.getsize(lp) > 30:\n",
        "        test_img = p\n",
        "        break\n",
        "\n",
        "thresholds = [0.10, 0.25, 0.50, 0.75]\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "for i, thresh in enumerate(thresholds):\n",
        "    img = np.array(Image.open(test_img))\n",
        "    axes[i].imshow(img, cmap='gray')\n",
        "    preds = best_model.predict(test_img, conf=thresh, verbose=False)\n",
        "    n_det = 0\n",
        "    if preds[0].boxes is not None:\n",
        "        for box in preds[0].boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            rect = patches.Rectangle(\n",
        "                (x1, y1), x2-x1, y2-y1,\n",
        "                linewidth=1.5, edgecolor='cyan', facecolor='none'\n",
        "            )\n",
        "            axes[i].add_patch(rect)\n",
        "            n_det += 1\n",
        "    axes[i].set_title(f'Conf ‚â• {thresh} ‚Üí {n_det} detections', fontsize=11)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Effect of Confidence Threshold on Detections',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e0frHElIYc8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚ùì Questions:**\n",
        "\n",
        "1. How does detection recall vary across the three size groups? Why are small craters harder to detect?\n",
        "2. What confidence threshold would you choose for (a) building a complete crater catalog vs. (b) safe landing site selection?\n",
        "3. Look at the gap between mAP@50 and mAP@50-95. What does this tell you about localization precision?"
      ],
      "metadata": {
        "id": "-iQ9VlRYYgQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 6: Optional Experiments\n",
        "\n",
        "Try one of these if time permits."
      ],
      "metadata": {
        "id": "oC7nS0U5YijV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT A: Larger model (YOLOv5s ‚Äî 11M params) (YOLOv5m ‚Äî 25M params) (YOLOv5l ‚Äî 43M params)\n",
        "# ============================================================\n",
        "# model_s = YOLO('yolov5s.pt')\n",
        "# results_s = model_s.train(\n",
        "#     data=yaml_path, epochs=40, imgsz=256, batch=32,\n",
        "#     name='martian_craters_small', patience=10, verbose=False\n",
        "# )\n",
        "# metrics_s = model_s.val(data=yaml_path, split='val')\n",
        "# print(f\"YOLOv8s mAP@50: {metrics_s.box.map50:.3f}\")"
      ],
      "metadata": {
        "id": "DIoFZhdEYeip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT B: More advanced model (YOLOv8n, YOLOv11n,)\n",
        "# ============================================================\n",
        "# model_s = YOLO('yolov8s.pt')\n",
        "# results_s = model_s.train(\n",
        "#     data=yaml_path, epochs=40, imgsz=256, batch=32,\n",
        "#     name='martian_craters_small', patience=10, verbose=False\n",
        "# )\n",
        "# metrics_s = model_s.val(data=yaml_path, split='val')\n",
        "# print(f\"YOLOv8s mAP@50: {metrics_s.box.map50:.3f}\")"
      ],
      "metadata": {
        "id": "NrbH58BCUhns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT C: Filter tiny craters\n",
        "# Remove craters < 5 pixels from training labels.\n",
        "# Does focusing on detectable craters improve overall mAP?\n",
        "# ============================================================\n",
        "# Hint: modify the build_yolo_split function to skip\n",
        "# annotations where max(bw, bh) * IMG_SIZE < 5"
      ],
      "metadata": {
        "id": "BnkHC5b_Yl3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìù Lab Wrap-Up & Discussion\n",
        "\n",
        "### Discussion Questions\n",
        "\n",
        "1. **Small object detection**: Nearly 40% of craters are ‚â§10 pixels. What architectural modifications could improve small-crater detection? (Hint: consider the Feature Pyramid Network, higher input resolution, or specialized anchor sizes.)\n",
        "\n",
        "2. **Transfer learning**: We transferred from COCO (everyday objects like cars and cats) to Martian craters. Why does this work? What features from COCO are useful for craters? Would starting from a model pretrained on satellite imagery be better?\n",
        "\n",
        "3. **Catalog quality**: The Robbins & Hynek catalog was manually compiled using THEMIS IR + topographic data. What are the implications of using the same imagery for both labeling and detection? Could the model find craters the catalog missed?\n",
        "\n",
        "4. **From Mars to the Moon**: How would you adapt this pipeline for lunar crater detection? What differences in imagery (LROC vs. THEMIS), crater morphology, and surface conditions would you need to account for?\n",
        "\n",
        "5. **Scientific validation**: If you deployed this globally on Mars, how would you validate the output? How could you estimate completeness and contamination rates as a function of crater size?\n",
        "\n",
        "\n",
        "### Going Further\n",
        "\n",
        "- Train on the **full 50K training set** for a research-grade model\n",
        "- Try **YOLOv8m** or **YOLOv8l** for higher accuracy (at the cost of speed)\n",
        "- Fuse THEMIS IR imagery with **MOLA topographic data** as additional input channels\n",
        "- Submit results to the [GeoAI challenge leaderboard](https://codalab.lisn.upsaclay.fr/competitions/1934)\n",
        "\n",
        "### Citations\n",
        "\n",
        "```bibtex\n",
        "@article{hsu2021knowledge,\n",
        "  title={Knowledge-Driven GeoAI: Integrating Spatial Knowledge into Multi-Scale\n",
        "         Deep Learning for Mars Crater Detection},\n",
        "  author={Hsu, Chia-Yu and Li, Wenwen and Wang, Sizhe},\n",
        "  journal={Remote Sensing},\n",
        "  volume={13}, number={11}, pages={2116},\n",
        "  year={2021}, publisher={MDPI}\n",
        "}\n",
        "\n",
        "@article{robbins2012new,\n",
        "  title={A new global database of Mars impact craters $\\geq$ 1 km:\n",
        "         1. Database creation, properties, and parameters},\n",
        "  author={Robbins, Stuart J and Hynek, Brian M},\n",
        "  journal={Journal of Geophysical Research: Planets},\n",
        "  volume={117}, number={E5}, year={2012}\n",
        "}\n",
        "\n",
        "@article{edwards2011mosaicking,\n",
        "  title={Mosaicking of global planetary image datasets: 1. Techniques and data\n",
        "         processing for THEMIS multi-spectral data},\n",
        "  author={Edwards, Christopher S and others},\n",
        "  journal={Journal of Geophysical Research: Planets},\n",
        "  volume={116}, number={E10}, year={2011}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "42-DntrTYrbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # EXPERIMENT D: Train on MORE data\n",
        "# # Increase N_TRAIN to 20000 or even 50000 (full dataset)\n",
        "# # and re-run Parts 3-5. Does more data help?\n",
        "# # ============================================================\n",
        "\n",
        "# # Download & Unzip FULL Martian Dataset\n",
        "\n",
        "# import os\n",
        "# import shutil\n",
        "# import zipfile\n",
        "# import gdown\n",
        "# import glob\n",
        "\n",
        "# DATA_DIR = '/content/MARTIAN_DATASET'\n",
        "# ZIP_PATH = os.path.join(DATA_DIR, \"2022_GeoAI_Martian_Challenge_Dataset.zip\")\n",
        "# EXTRACT_DIR = os.path.join(DATA_DIR, \"extracted\")\n",
        "\n",
        "# if not os.path.exists(DATA_DIR):\n",
        "#     os.makedirs(DATA_DIR)\n",
        "\n",
        "# print(\"üöÄ Starting download from Google Drive ...\")\n",
        "\n",
        "# file_ids = {\n",
        "#     '2022_GeoAI_Martian_Challenge_Dataset.zip': '1eGCBMeyDzKL7DNk01qSbqOrqpLOnQiR6',\n",
        "# }\n",
        "\n",
        "# for filename, file_id in file_ids.items():\n",
        "#     output_path = os.path.join(DATA_DIR, filename)\n",
        "#     if not os.path.exists(output_path):\n",
        "#         print(f\"   Downloading {filename}...\")\n",
        "#         gdown.download(id=file_id, output=output_path, quiet=False)\n",
        "#     else:\n",
        "#         print(f\"   {filename} already exists, skipping download.\")\n",
        "\n",
        "# # Extract\n",
        "# if not os.path.exists(EXTRACT_DIR):\n",
        "#     print(\"Extracting dataset...\")\n",
        "#     !unzip -q \"{ZIP_PATH}\" -d \"{EXTRACT_DIR}\"\n",
        "# else:\n",
        "#     print(\"Already extracted.\")\n",
        "\n",
        "# # Find the root of the extracted data\n",
        "# # It may be nested inside a subfolder\n",
        "# candidates = glob.glob(os.path.join(EXTRACT_DIR, '**', 'ids.json'), recursive=True)\n",
        "# if candidates:\n",
        "#     CHALLENGE_DIR = os.path.dirname(candidates[0])\n",
        "# else:\n",
        "#     # Try direct path\n",
        "#     CHALLENGE_DIR = EXTRACT_DIR\n",
        "\n",
        "# print(f\"Dataset root: {CHALLENGE_DIR}\")\n",
        "# print(\"Contents:\")\n",
        "# if os.path.exists(CHALLENGE_DIR):\n",
        "#     for item in sorted(os.listdir(CHALLENGE_DIR)):\n",
        "#         full = os.path.join(CHALLENGE_DIR, item)\n",
        "#         if os.path.isdir(full):\n",
        "#             n_files = len(os.listdir(full))\n",
        "#             print(f\"  üìÅ {item}/ ({n_files:,} files)\")\n",
        "#         else:\n",
        "#             print(f\"  üìÑ {item} ({os.path.getsize(full)/1e6:.1f} MB)\")\n",
        "# else:\n",
        "#     print(f\"‚ùå Error: Directory {CHALLENGE_DIR} does not exist. Extraction might have failed.\")\n",
        "\n",
        "\n",
        "\n",
        "# # @title Load the split IDs and annotations\\\n",
        "# import json\n",
        "# import os\n",
        "# import numpy as np\n",
        "\n",
        "# with open(os.path.join(CHALLENGE_DIR, 'ids.json'), 'r') as f:\n",
        "#     ids = json.load(f)\n",
        "\n",
        "# with open(os.path.join(CHALLENGE_DIR, 'gt_public.json'), 'r') as f:\n",
        "#     gt_public = json.load(f)\n",
        "\n",
        "# print(f\"Split sizes:\")\n",
        "# for split_name in ['train', 'val', 'test']:\n",
        "#     print(f\"  {split_name:6s}: {len(ids.get(split_name, [])):>6,} images\")\n",
        "\n",
        "# print(f\"\\nAnnotation file keys: {list(gt_public.keys())[:5]}...\")\n",
        "# print(f\"  Images with annotations:      {len(gt_public):,}\")\n",
        "# print(f\"  Total annotations (bounding boxes): {sum(len(v) for v in gt_public.values()):,}\")\n",
        "# print(f\"  Categories: ['crater'] \")\n",
        "\n",
        "\n",
        "# sample_image_id = next(iter(gt_public))\n",
        "# print(f\"Sample Image ID: {sample_image_id}\")\n",
        "# print(json.dumps(gt_public[sample_image_id][:2], indent=2))\n",
        "\n",
        "# print(\"\\n‚Üí Bounding box format: [x_min, y_min, width, height] in pixels (COCO format)\")\n",
        "# print(\"‚Üí We need to convert this to YOLO format: (x_center, y_center, width, height) normalized to [0,1]\")\n",
        "\n",
        "# # image_id ‚Üí image info\n",
        "# # The gt_public.json provided for this lab contains only image_id -> annotations.\n",
        "# # We need to construct image info based on the fixed image size and known image IDs.\n",
        "# # All images are 256x256 as per the lab description.\n",
        "# img_info = {}\n",
        "# # Collect all unique image IDs from the splits to ensure all images have info\n",
        "# all_image_ids = set(ids['train']) | set(ids['val']) | set(ids['test'])\n",
        "# for img_id in all_image_ids:\n",
        "#     img_info[img_id] = {\n",
        "#         'id': img_id,\n",
        "#         'file_name': f\"{img_id}.png\",\n",
        "#         'width': 256,\n",
        "#         'height': 256\n",
        "#     }\n",
        "\n",
        "# # image_id ‚Üí list of annotations\n",
        "# from collections import defaultdict\n",
        "# # Populate img_anns using the gt_public directly, as it already maps image IDs to lists of annotations\n",
        "# img_anns = defaultdict(list)\n",
        "# for img_id, annotations_list in gt_public.items():\n",
        "#     img_anns[img_id] = annotations_list\n",
        "\n",
        "# # Quick stats on training annotations\n",
        "# train_ids_set = set(ids['train'])\n",
        "# train_crater_counts = []\n",
        "# train_crater_sizes = []\n",
        "\n",
        "# for img_id in ids['train']:\n",
        "#     # Only consider images that actually have annotations in img_anns\n",
        "#     if img_id in img_anns:\n",
        "#         anns = img_anns[img_id]\n",
        "#         train_crater_counts.append(len(anns))\n",
        "#         for a in anns:\n",
        "#             # Bounding box format is [x_min, y_min, w, h]\n",
        "#             bw, bh = a[2], a[3]\n",
        "#             train_crater_sizes.append(max(bw, bh))  # diameter in pixels\n",
        "\n",
        "# print(f\"Training set statistics:\")\n",
        "# print(f\"  Total craters: {sum(train_crater_counts):,}\")\n",
        "# print(f\"  Mean craters/image: {np.mean(train_crater_counts):.1f}\")\n",
        "# print(f\"  Crater size (pixels): median={np.median(train_crater_sizes):.0f}, \"\n",
        "#       f\"range=[{np.min(train_crater_sizes):.0f}, {np.max(train_crater_sizes):.0f}]\")\n",
        "\n",
        "\n",
        "# # Visualize dataset statistics\n",
        "# fig, axes = plt.subplots(1, 3, figsize=(18, 4.5))\n",
        "\n",
        "# # Craters per image\n",
        "# axes[0].hist(train_crater_counts, bins=range(0, max(train_crater_counts[:10000])+2),\n",
        "#              color='steelblue', edgecolor='white', linewidth=0.3)\n",
        "# axes[0].set_xlabel('Craters per image')\n",
        "# axes[0].set_ylabel('Number of images')\n",
        "# axes[0].set_title('Crater Count Distribution')\n",
        "# axes[0].set_xlim(0, 20)\n",
        "# axes[0].axvline(np.mean(train_crater_counts), color='red', ls='--',\n",
        "#                 label=f'Mean: {np.mean(train_crater_counts):.1f}')\n",
        "# axes[0].legend()\n",
        "\n",
        "# # Crater size distribution (pixels)\n",
        "# axes[1].hist(train_crater_sizes, bins=np.arange(0, 260, 5),\n",
        "#              color='coral', edgecolor='white', linewidth=0.3)\n",
        "# axes[1].set_xlabel('Crater diameter (pixels)')\n",
        "# axes[1].set_ylabel('Count')\n",
        "# axes[1].set_title('Crater Size Distribution')\n",
        "# axes[1].axvline(10, color='blue', ls='--', label='Small/Medium (10 px)')\n",
        "# axes[1].axvline(50, color='green', ls='--', label='Medium/Large (50 px)')\n",
        "# axes[1].legend(fontsize=9)\n",
        "\n",
        "# # Size groups pie chart\n",
        "# small = sum(1 for s in train_crater_sizes if s <= 10)\n",
        "# medium = sum(1 for s in train_crater_sizes if 10 < s <= 50)\n",
        "# large = sum(1 for s in train_crater_sizes if s > 50)\n",
        "# axes[2].pie([small, medium, large], labels=['Small\\n(‚â§10 px)', 'Medium\\n(11-50 px)', 'Large\\n(>50 px)'],\n",
        "#             colors=['#e74c3c', '#f39c12', '#3498db'], autopct='%1.1f%%',\n",
        "#             textprops={'fontsize': 11})\n",
        "# axes[2].set_title('Crater Size Groups')\n",
        "\n",
        "# plt.suptitle('GeoAI Martian Challenge ‚Äî Training Set Statistics',\n",
        "#              fontsize=14, fontweight='bold')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # ============================================================\n",
        "# # Configuration\n",
        "# # ============================================================\n",
        "# N_TRAIN = 5000     # Subset size for training (full: ~50K)\n",
        "# N_VAL = 1000       # Use the entire val set\n",
        "# N_TEST = 1000      # Subset for testing\n",
        "# IMG_SIZE = 256     # Image dimensions\n",
        "# YOLO_DIR = \"/content/martian_yolo\"\n",
        "\n",
        "# print(f\"Building YOLO dataset: {N_TRAIN} train / {N_VAL} val / {N_TEST} test images\")\n",
        "# print(f\"(Full dataset has ~50K train ‚Äî use for research projects)\")\n",
        "\n",
        "\n",
        "# def coco_to_yolo_box(bbox, img_w, img_h):\n",
        "#     \"\"\"Convert COCO bbox [x_min, y_min, w, h] to YOLO [xc, yc, w, h] normalized.\"\"\"\n",
        "#     x_min, y_min, bw, bh = bbox\n",
        "#     xc = (x_min + bw / 2.0) / img_w\n",
        "#     yc = (y_min + bh / 2.0) / img_h\n",
        "#     w = bw / img_w\n",
        "#     h = bh / img_h\n",
        "#     # Clamp to [0, 1]\n",
        "#     xc = max(0, min(1, xc))\n",
        "#     yc = max(0, min(1, yc))\n",
        "#     w = max(0.001, min(1, w))\n",
        "#     h = max(0.001, min(1, h))\n",
        "#     return xc, yc, w, h\n",
        "\n",
        "\n",
        "# def build_yolo_split(split_name, image_ids, n_images, yolo_dir, challenge_dir,\n",
        "#                      img_info, img_anns, img_size=256):\n",
        "#     \"\"\"\n",
        "#     Sample n_images from image_ids, copy images and create YOLO label files.\n",
        "#     Prioritizes images WITH annotations.\n",
        "#     \"\"\"\n",
        "#     img_dir = os.path.join(yolo_dir, split_name, 'images')\n",
        "#     lbl_dir = os.path.join(yolo_dir, split_name, 'labels')\n",
        "#     os.makedirs(img_dir, exist_ok=True)\n",
        "#     os.makedirs(lbl_dir, exist_ok=True)\n",
        "\n",
        "#     # Prefer images with annotations, but include some without\n",
        "#     with_anns = [i for i in image_ids if len(img_anns.get(i, [])) > 0]\n",
        "#     without_anns = [i for i in image_ids if len(img_anns.get(i, [])) == 0]\n",
        "\n",
        "#     # Sample: 90% with craters, 10% negatives\n",
        "#     n_pos = min(int(n_images * 0.9), len(with_anns))\n",
        "#     n_neg = min(n_images - n_pos, len(without_anns))\n",
        "\n",
        "#     selected = random.sample(with_anns, n_pos)\n",
        "#     if n_neg > 0 and without_anns:\n",
        "#         selected += random.sample(without_anns, n_neg)\n",
        "#     random.shuffle(selected)\n",
        "\n",
        "#     total_craters = 0\n",
        "#     for img_id in selected:\n",
        "#         info = img_info.get(img_id)\n",
        "#         if info is None:\n",
        "#             continue\n",
        "\n",
        "#         fname = info['file_name']\n",
        "#         src_path = os.path.join(challenge_dir, 'images', fname)\n",
        "#         if not os.path.exists(src_path):\n",
        "#             src_path = os.path.join(challenge_dir, 'images', os.path.basename(fname))\n",
        "#         if not os.path.exists(src_path):\n",
        "#             continue\n",
        "\n",
        "#         # Copy image\n",
        "#         dst_img = os.path.join(img_dir, os.path.basename(fname))\n",
        "#         shutil.copy2(src_path, dst_img)\n",
        "\n",
        "#         # Write YOLO labels\n",
        "#         stem = Path(fname).stem\n",
        "#         lbl_path = os.path.join(lbl_dir, f\"{stem}.txt\")\n",
        "#         anns = img_anns.get(img_id, [])\n",
        "\n",
        "#         with open(lbl_path, 'w') as f:\n",
        "#             for a in anns:\n",
        "#                 xc, yc, w, h = coco_to_yolo_box(a, img_size, img_size)\n",
        "#                 f.write(f\"0 {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\\n\")\n",
        "#                 total_craters += 1\n",
        "\n",
        "#     return len(selected), total_craters\n",
        "\n",
        "\n",
        "# print(\"Building YOLO dataset...\")\n",
        "\n",
        "\n",
        "# %%time\n",
        "# import random\n",
        "# import json\n",
        "# import os\n",
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Also load gt_eval.json if available (contains val annotations)\n",
        "# eval_path = os.path.join(CHALLENGE_DIR, 'gt_eval.json')\n",
        "# if os.path.exists(eval_path):\n",
        "#     with open(eval_path, 'r') as f:\n",
        "#         gt_eval = json.load(f)\n",
        "#     # Merge eval annotations into our lookup\n",
        "#     for img_entry in gt_eval.get('images', []):\n",
        "#         if img_entry['id'] not in img_info:\n",
        "#             img_info[img_entry['id']] = img_entry\n",
        "#     for ann in gt_eval.get('annotations', []):\n",
        "#         img_anns[ann['image_id']].append(ann)\n",
        "#     print(f\"Loaded gt_eval.json: {len(gt_eval.get('annotations', []))} annotations\")\n",
        "\n",
        "# # --- FIX: Create a local test set from unused training images ---\n",
        "# # The official 'test' set has no labels (blind test set for competition).\n",
        "# # We will use a slice of the 'train' pool that wasn't used for training as our 'test' set.\n",
        "\n",
        "# # Shuffle train IDs to ensure random selection\n",
        "# all_train_ids = ids['train']\n",
        "# random.shuffle(all_train_ids)\n",
        "\n",
        "# # Split: Train (0 to N_TRAIN) | Test (N_TRAIN to N_TRAIN + N_TEST)\n",
        "# train_split_ids = all_train_ids[:N_TRAIN]\n",
        "# test_split_ids  = all_train_ids[N_TRAIN : N_TRAIN + N_TEST]\n",
        "# val_split_ids   = ids['val'] # Keep official val set\n",
        "\n",
        "# print(f\"Redefined splits to ensure Test set has labels:\")\n",
        "# print(f\"  Train source: ids['train'][:{N_TRAIN}]\")\n",
        "# print(f\"  Test source:  ids['train'][{N_TRAIN}:{N_TRAIN+N_TEST}]\")\n",
        "\n",
        "# # Build each split\n",
        "# stats = {}\n",
        "# for split_name, split_ids, n in [('train', train_split_ids, N_TRAIN),\n",
        "#                                    ('val', val_split_ids, N_VAL),\n",
        "#                                    ('test', test_split_ids, N_TEST)]:\n",
        "#     n_imgs, n_craters = build_yolo_split(\n",
        "#         split_name, split_ids, n, YOLO_DIR, CHALLENGE_DIR,\n",
        "#         img_info, img_anns, IMG_SIZE\n",
        "#     )\n",
        "#     stats[split_name] = (n_imgs, n_craters)\n",
        "#     print(f\"  {split_name:6s}: {n_imgs:>5,} images, {n_craters:>6,} craters\")\n",
        "\n",
        "# print(\"\\n‚úÖ YOLO dataset ready!\")\n",
        "\n",
        "\n",
        "# # Write data.yaml for YOLO\n",
        "# import os\n",
        "# import yaml\n",
        "\n",
        "# # Ensure YOLO_DIR is defined\n",
        "# YOLO_DIR = \"/content/martian_yolo\"\n",
        "\n",
        "# data_yaml = {\n",
        "#     'path': YOLO_DIR,\n",
        "#     'train': 'train/images',\n",
        "#     'val': 'val/images',\n",
        "#     'test': 'test/images',\n",
        "#     'nc': 1,\n",
        "#     'names': ['crater'],\n",
        "# }\n",
        "\n",
        "# yaml_path = os.path.join(YOLO_DIR, 'data.yaml')\n",
        "# with open(yaml_path, 'w') as f:\n",
        "#     yaml.dump(data_yaml, f, default_flow_style=False)\n",
        "\n",
        "# print(f\"Created config at: {yaml_path}\")\n",
        "# print(open(yaml_path).read())\n"
      ],
      "metadata": {
        "id": "wbYAv-RsqwkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}